---
marp: true
title: Mysteries of parallelization
size: 4:3
paginate: true

theme: default

style: |
  section {
    background-color: #fff;
    justify-content: normal;
  }

  section.lead {
    background-color: #fff;
    justify-content: center;
  }

  section.lead h1, section.lead h2, section.lead h3, section.lead h4{
    text-align: center;
  }

  section.lead p {
    text-align: center;
  }

  img[alt=width50] {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 50%;
  }
  img[alt=c10em] {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 10em;
  }
  img[alt=c15em] {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 15em;
  }
  img[alt=c20em] {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 20em;
  }
  img[alt=c25em] {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 25em;
  }

---

<!-- _paginate: false -->
<!-- _class: lead -->

# Распараллеливание массовых вычислений

</br>

### С.А.Романенко</br>

ИПМ им. М.В. Келдыша РАН, Москва<br/>
28 февраля 2021

---

<!-- _class: lead -->

## Появление новых типов вычислительных устройств<br/>:sunrise:

<br/><br/><br/>

---

## Застой в информатике

В информатике наблюдался застой где-то с середины
70-х годов.

**Причины**:

- Безраздельно доминировал один тип вычислительных устройств:
  фон-неймановская машина (ФНМ).

- Необычайная гибкость фон-неймановской машины делала жизнь
программистов слишком лёгкой. 

---

## Специфические свойства ФНМ

![c10em](single-cpu.dot.svg)

- Последовательное исполнение команд.
- Большая и однородная память.
- Данные неподвижно лежат в том месте, куда их положили.

---

## Специфические свойства ФНМ

- Последовательное исполнение команд.
  - Нет проблем с синхронизацией. Если надо решить 2 задачи, то
    решаем сначала 1-ю, а потом -- 2-ю (или наоборот).

- Большая и однородная память.
  - Из любой части программы можно в любой момент "протянуть руку" к любому
    месту памяти.
  - Данные неподвижно лежат в том месте, куда их положили. Поэтому,
    одни данные могут ссылаться на другие. А с помощью ссылок можно
    создавать сложные структуры данных (списки, деревья).

---

## Виды параллелизма

Параллелизм, это когда **много задач решаются одновременно**.

Два основных варианта:

- Данные - общие, задачи - разные.

- Данные - разные, задача - одна и та же. (= **"Массовый"
  параллелизм.**)

(Конечно, это - упрощенная картина.)

---

## Разнородные задачи

![c15em](parall_diverse_tasks.dot.svg)

Для одних и тех же данных одновременно решается много
**_разных_** задач.

(Пример - параллельная обработка многих несвязанных запросов к безе данных.)

---

## Однородные задачи

![c15em](parall_similar_tasks.dot.svg)

**_Одна и та же_** задача решается для разных данных
  (**_массовый_** параллелизм).

(Пример - повышение резкости изображения, когда одни и те же
действия выполянются для каждой точки изображения.)

---

## Параллельные вычислители

- Многопроцессорность (multiprocessing).
  - Много процессоров (на плате или в шкафу).
  - Память у процессоров, может быть, общая, а, может быть,
    и нет.
- Многоядерность (multi-core processsor).
  - Несколько процессоров (ядер) в одном чипе.
  - Память для всех ядер - общая.
- Векторность (SIMD = single instruction, multiple data).
  - Одна команда может исполняться над несколькими данными.

---

## Многопроцессорность (multiprocessing)

![c20em](multi-cpu.dot.svg)

- Много процессоров (на плате или в шкафу).
- Память у процессоров, может быть, общая, а, может быть,
  и нет.

---

## Векторность (SIMD)

SIMD = single instruction, multiple data).

![c15em](simd.dot.svg)

- Одна команда может исполняться над несколькими данными.

---

## Графический процессор (ГП, GPU)

![c15em](gpu.dot.svg)

- Много-много процессоров (PU), разбитых на группы,
  у каждой из которых есть своя быстрая память (SRAM = shared RAM).
- И есть общая для всех память (DRAM).

---

<!-- _class: lead -->

## Графический процессор -<br/>это<br>"суперкомпьютер для бедных"<br/>:grin:

<br/><br/><br/>

---

## ГП: много-много нитей (threads)

![c20em](gbwt.dot.svg)

Grid -> Block -> Warp -> Thread

- Решётка (grid) состоит из блоков.
- Блок (block) состоит из жгутов/пучков.
- Жгут (warp) состоит из нитей.

> Чем ниже по иерархии, тем **более тесное** взаимодействие
  возможно между нитями!

---

## ГП: зачем нужна иерархия для нитей?

### Блоки

- Внутри блока возможна синхронизация нитей (барьеры).
- Внутри блока нити могут работать с быстрой общей
  (shared) памятью.

---

## ГП: зачем нужна иерархия для нитей?

### Жгуты

- Все нити внутри жгута исполняют одну и ту же команду
  (SIMT = single instruction - multiple threads).
  Но некоторые нити при этом могут простаивать.
- Не нужно барьерная синхронизация внутри жгута, поскольку
  управление для всех нитей и так движется синхронно.

---

## ГП: Nvidia GeForce GT 1030

 ~7000 ₽

![c20em](GeForce_GT_1030.jpg)

---

## ГП: Nvidia GeForce GT 1030

<style scoped>
  td {
    font-size: 0.80em;
    color: green;
  }
</style>

&nbsp;                        | &nbsp;
------------------------------|----------------
Device Name                   | GeForce GT 1030
Number of Multiprocessors     | 3
Max Threads Per SMP           | 2048
Maximum Threads per Block     | 1024
Warp Size                     | 32
Global Memory Size            | 2095251456
Total Constant Memory         | 65536
Total Shared Memory per Block | 49152
Registers per Block           | 65536
L2 Cache Size                 | 524288 bytes

---

## ГП - это "микро-супер-компьютер"!

- "Блоки" соответствуют "узлам".
- "Жгуты" соответствуют "процессорам" с векторными операциями
  (SIMD).

Результат - "демократизация" массового параллелизма.

Пример: компьютерная томография. Раньше это было нечто элитарное
(требовался суперкомпьютер), а теперь томограф - это шататное
оборудование...

---

## А как же программируемая логика (ПЛИС, FPGA)?

А дело в том, что ПЛ, это **не архитектура**, а **средство
реализации** различных архитектур.

- Реализовать архитектуру можно только если она уже придумана.
- Если взять одну из известных архитектур, то для неё уже есть
  готовые реализации.
- Но можно придумать новую, неслыханную архитектуру - и реализовать
  прототип с помощью ПЛ. А потом - заказать чипы.

---

## Недостатки ПЛ с точки зрения программиста

- Изобретение новых архитектур - это не программирование,
  а несколько иная специальность.
- А если использовать известную архитектуру, то реализовывать
  её через ПЛ - нецелесообразно (дорого и медленно работает).
- Компиляция программ для известных архитектур происходит
  за секунды, а синтез схем для ПЛ может тянуться часами.
  (Нечеловеческие условия работы... :grimacing:)

---

## Почему жить стало интереснее?

- Изменилась "система ценностей" в теории сложности вычислений.
  - Раньше цель состояла в уменьшении числа операции (что
    автоматически уменьшало время выполнения). А теперь,
    время выполнения и число операций (= затраты электроэнергии) -
    не одно и то же.
- Методы и алгоритмы, которые являются "хорошими" при
  последовательных вычислениях, не обязательно являются таковыми
  в случае параллельных вычислений.

---

## Что можно "распараллеливать"?

- Программу.
  - Вставляем директивы для компилятора.
  - Реорганизуем программу.

- Алгоритм.
  - Выявляем скрытый параллелизм.
  - Реорганизуем алгоритм.
  - **Заменяем на другой алгоритм.**

- Метод.
  - Изменяем или заменяем метод.

---

<!-- _class: lead -->

## Как программировать "параллельно"<br/>:question:

<br/><br/><br/>

---

# Средства программирования для ГП NVIDIA

- **CUDA Toolkit**.
  - <https://developer.nvidia.com/cuda-toolkit>
  - C/C++ с расширениями (дополнительными ключевыми словами).
- **OpenACC** (часть NVIDIA HPC SDK).
  - <https://developer.nvidia.com/openacc>
  - C/C++ с директивами `#pragma acc`. Можно компилировать
    программу игнорируя эти директивы и исполнять программу
    на обычном ЦП.

---

## Взаимодействие ЦП и ГП

![c25em](host_accel.dot.svg)

- ЦП работает с RAM.
- ГП работает с DRAM.
  - DRAM - быстрее, чем RAM, но меньше.
- Данные переходится пересылать между RAM и DRAM.

---

## Пересылка данных между RAM и DRAM

### CUDA

```
size_t size = numElements * sizeof(int);
int *h_a = (int *) malloc(size);
cudaMalloc(&d_a, size);
cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
cudaMemcpy(h_a, d_a, size, cudaMemcpyDeviceToHost);
```

### OpenACC

```
size_t size = numElements * sizeof(int);
int *a = new int[numElements];
#pragma acc enter data copyin(a[0:numElements])
#pragma acc exit data copyout(a[0:numElements])
```

---

<!-- _class: lead -->

## Использование CUDA

### <https://developer.nvidia.com/cuda-toolkit>

<br/><br/><br/>

---

## Задача - сложение векторов

```cpp
void vector_add_cpu(int const n,
    int const *a, int const *b, int *r) {
  for (int i = 0; i < n; i++)
    r[i] = a[i] + b[i];
}
```

Хорошо поддаётся распараллеливанию. Почему?

- i-й проход цикла **не использует** результаты работы предыдущих
  проходов цикла!
- ⟹ Все проходы цикла можно выполнять в **произвольном
  порядке**.
- ⟹ Все проходы цикла можно исполнять **одновременно**.

---

## CUDA. Cложение векторов - ядро

- Исполнение цикла в CUDA - каждый проход цикла исполняется
  с помощью отдельной нити.
- Порядок исполнения нитей - произвольный.
- Ядро может запросить номер своей нити.


```cpp
__global__
void vector_add_kernel(int const n,
    int const *a, int const *b, int *r) {
  int i = threadIdx.x + blockDim.x * blockIdx.x;

  if (i < n) {
    r[i] = a[i] + b[i];
  }
}
```

---

## CUDA. Работа с номером нити

```cpp
  int i = threadIdx.x + blockDim.x * blockIdx.x;

  if (i < n) {
    ...
  }
```

- `threadIdx.x` - номер нити внутри блока.
- `blockIdx.x` - номер блока.
- `blockDim.x` - размер блока

**Вопрос.** Зачем нужна проверка `i < n`?

**Ответ.** Число нитей = `blockDim.x * gridDim.x`,
и может быть **больше** `n`.

---

## CUDA. Запуск нитей

```cpp
void vector_add_gpu(int const n,
    int const *a, int const *b, int *r) {
  int TPB = 256; // Threads per block.
  int BPG = (n + TPB - 1) / TPB; // Blocks per grid
  vector_add_kernel<<<BPG, TPB>>>(n, a, b, r);
}
```

- Запускаем `BPG` блоков, в каждом из которых - `TPB` нитей.
- Выражение `(n + TPB - 1) / TPB` - это вычурный способ выполнить
  "деление с округлением вверх". Например:  
  (257 + 256 - 1) / 256 = 512 / 2 = 2.


---

## CUDA. Достоинства

- Можно программировать на широкоизвестном C/C++.
- Последовательная программа довольно легко переделывается в
  CUDA-программу (если она **уже является** параллельной по сути).
- Можно учитывать тонкие свойства "железа" и использовать
  операции низкого уровня.

---

## CUDA. Недостатки

- Приходится работать в понятиях, отражающих устройство "железа",
  а не алгоритма (с нитями, а не с переменной цикла).
- CUDA-программа не может быть скомпилирована обычным C/C++
  компилятором и исполнена как последовательная.
- CUDA-программа предназначена для ГП, и её нельзя исполнить на
  многоядерном процессоре.

А хочется **иметь один текст программы**, который можно было бы
компилировать и исполнять для **разных архитектур**!

---

<!-- _class: lead -->

## Использование OpenACC

### <https://developer.nvidia.com/openacc>

<br/><br/><br/>

---

## OpenACC. Сложение векторов

```
void vector_add_acc(int const n,
  int const *a, int const *b, int *r) {
#pragma acc parallel loop \
    copyin(a[:n], b[:n]) copyout(r[:n])
  for (int i = 0; i < n; i++)
    r[i] = a[i] + b[i];
}
```

- Из одного и того же текста генерируются исполняемые
  программы и для ЦП, и для многоядерного ЦП и для ГП.
- `parallel loop` ~ тело цикла для
  разных `i` можно исполлнять одновременно.
- `copyin` и `copyout` ~ пересылки между RAM и DRAM.

---

## OpenACC. Достоинства

- **Единый** текст программы для **разных архитектур**.
- Можно написать и отладить последовательную программу для ЦП,
  а **потом добавлять** в неё директивы `#pragma acc`.
- Программист может мыслить в понятиях, **не привязанных к мелким
  деталям устройства ГП**. Например, компилятор сам решает, как
  отобразить переменную цикла `i` на нити, жгуты и блоки.
- Можно компилировать не только для ГП от Nvidia, но и для
  ГП от AMD.

---

## OpenACC. Недостатки

- Нельзя использовать некоторые средства низкого уровня,
  предоставляемые ГП от конкретного производителя.
  - Однако, можно некоторые части программы написать на CUDA,
    а потом вызвать их из OpenACC-программы.
- OpenACC-программы хорошо писать, когда уже освоено программирование
  для ГП на более низком уровне (на CUDA), ибо без этого сложно
  понимать смысл директив `#pragma acc` и смысл некоторых сообщений
  компилятора (выдаваемых в терминах CUDA).

---

## Разное

- "Встреча у фонтана".
